# Job_A: RLM Orchestrator (Control Plane)
#
# Responsibilities:
#   - Load secrets/config
#   - Create/continue sessions in UC via DeltaSessionService
#   - Generate executable artifacts into UC Volumes
#   - Submit Job_B (executor) runs via Jobs API
#   - Record submission metadata and high-level telemetry
#
# Resource key MUST remain stable for stable job IDs across redeploys.

resources:
  jobs:
    rlm_orchestrator_job:
      name: "[${bundle.target}] RLM Orchestrator"
      description: "RLM Agent orchestrator - control plane for the two-job pattern"
      
      # Run on existing cluster (no cluster creation permissions)
      tasks:
        - task_key: run_orchestrator
          existing_cluster_id: ${var.cluster_id}
          
          python_wheel_task:
            package_name: databricks_rlm_agent
            entry_point: rlm-orchestrator
            # CLI args for static config; dynamic job params (ADK_PROMPT etc.) are
            # fetched at runtime via spark.conf or dbutils by the CLI itself
            parameters:
              - "--catalog=${var.uc_catalog}"
              - "--schema=${var.uc_schema}"

          # Libraries - the wheel artifact built by the bundle
          libraries:
            - whl: ../databricks_rlm_agent/dist/databricks_rlm_agent-*.whl

      # Environment variables for the job
      # Note: spark_env_vars would go inside new_cluster config
      # For existing_cluster_id, env vars are set via task parameters or dbutils
      parameters:
        - name: ADK_DELTA_CATALOG
          default: ${var.uc_catalog}
        - name: ADK_DELTA_SCHEMA
          default: ${var.uc_schema}
        - name: ADK_ARTIFACTS_PATH
          default: ${var.artifacts_path}
        - name: ADK_EXECUTOR_JOB_ID
          default: ""  # Will be set by deploy script after both jobs exist
        - name: ADK_SECRET_SCOPE
          default: ${var.secret_scope}
        - name: ADK_PROMPT
          default: ""  # User prompt - passed via run-now or trigger
        - name: ADK_PROMPT_FILE
          default: "/Volumes/silo_dev_rs/task/task_txt/task.txt"  # Path to prompt file (used if ADK_PROMPT is empty)
        - name: ADK_SESSION_ID
          default: "session_001"
        - name: ADK_USER_ID
          default: "job_user"
        - name: ADK_MAX_ITERATIONS
          default: "30"
        - name: TEST_LEVEL
          default: ""  # Test task level (1-10) - bypasses ingestor polling for E2E testing
        # Model provider configuration
        - name: ADK_MODEL_PROVIDER
          default: "litellm"  # "gemini" (native ADK) or "litellm" (LiteLLM multi-provider)
        - name: ADK_GEMINI_MODEL
          default: "gemini-3-pro-preview"  # Model for native Gemini provider (unused when litellm)
        - name: ADK_LITELLM_MODEL
          default: "openai/gpt-5.2"  # Primary model for LiteLLM provider (OpenAI model id: gpt-5.2; usable via Chat Completions or Responses API)
        - name: ADK_LITELLM_FALLBACK_MODELS
          default: ""  # Comma-separated fallback chain (e.g., "openai/gpt-4o-mini,anthropic/claude-3-haiku-20240307")
        - name: ADK_FALLBACK_ON_BLOCKED
          default: "true"  # Enable fallback on content-policy errors
        - name: ADK_FALLBACK_GEMINI_TO_LITELLM
          default: "true"  # Enable Gemini->LiteLLM fallback on blocking errors
        # Timeout configuration
        - name: ADK_TIMEOUT_SECONDS
          default: "3600"  # Total conversation timeout (1 hour)
        - name: ADK_EVENT_TIMEOUT_SECONDS
          default: "600"  # Event timeout between stream events (10 minutes)

      # Job settings
      max_concurrent_runs: 1
      timeout_seconds: 7200  # 2 hours

      # Email notifications (optional - configure as needed)
      # email_notifications:
      #   on_failure:
      #     - your-email@example.com

      # Tags for organization
      tags:
        project: rlm-agent
        component: orchestrator
        bundle: ${bundle.name}

