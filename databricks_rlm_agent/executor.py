"""Executor harness for Job_B (Execution Plane).

This module provides the artifact execution logic for Job_B. It:
- Loads and executes generated Python artifacts from UC Volumes
- Supports loading code from artifact registry by artifact_id
- Captures stdout/stderr during execution for RLM feedback loop
- Writes result.json to Volumes with captured output
- Prints RLM markers for log parsing by job_builder
- Records telemetry

The executor is designed to run code generated by Job_A in a controlled
environment with proper error handling and result capture.

RLM Markers:
    Output is wrapped in markers for reliable parsing:
    ===RLM_EXEC_START artifact_id={id}===
    <agent code output>
    ===RLM_EXEC_END artifact_id={id} status={success|failed}===
"""

import io
import json
import logging
import os
import sys
import traceback
from contextlib import redirect_stdout, redirect_stderr
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Optional, TYPE_CHECKING

if TYPE_CHECKING:
    from pyspark.sql import SparkSession

logger = logging.getLogger(__name__)


def execute_artifact(
    spark: "SparkSession",
    artifact_path: str,
    run_id: str,
    iteration: int,
    catalog: str,
    schema: str,
    timeout_seconds: int = 3600,
) -> dict[str, Any]:
    """Execute an artifact from UC Volumes with stdout/stderr capture.
    
    Args:
        spark: SparkSession for execution context.
        artifact_path: Full path to the artifact in UC Volumes.
        run_id: The orchestrator's run identifier.
        iteration: The current iteration number.
        catalog: Unity Catalog name.
        schema: Schema name.
        timeout_seconds: Maximum execution time (not enforced in this version).
        
    Returns:
        Dict with execution results:
        - status: "success" or "failed"
        - artifact_path: The path that was executed
        - run_id: The run identifier
        - iteration: The iteration number
        - start_time: ISO timestamp of execution start
        - end_time: ISO timestamp of execution end
        - duration_seconds: Execution duration
        - stdout: Captured standard output from execution
        - stderr: Captured standard error from execution
        - output: Any captured output variable (if applicable)
        - error: Error message (if failed)
        - error_trace: Full traceback (if failed)
    """
    start_time = datetime.now(timezone.utc)
    
    logger.info(f"Executing artifact: {artifact_path}")
    logger.info(f"Run ID: {run_id}, Iteration: {iteration}")
    
    result = {
        "status": "pending",
        "artifact_path": artifact_path,
        "run_id": run_id,
        "iteration": iteration,
        "start_time": start_time.isoformat(),
        "end_time": None,
        "duration_seconds": None,
        "stdout": None,
        "stderr": None,
        "output": None,
        "error": None,
        "error_trace": None,
    }
    
    # Create buffers for capturing stdout/stderr
    stdout_buffer = io.StringIO()
    stderr_buffer = io.StringIO()
    
    try:
        # Validate artifact path
        if not artifact_path:
            raise ValueError("No artifact path provided")
        
        if not os.path.exists(artifact_path):
            raise FileNotFoundError(f"Artifact not found: {artifact_path}")
        
        # Read the artifact
        logger.info(f"Reading artifact from: {artifact_path}")
        with open(artifact_path, 'r') as f:
            code = f.read()
        
        logger.info(f"Artifact loaded, {len(code)} characters")
        
        # Prepare execution environment
        exec_globals = {
            "__name__": "__main__",
            "__file__": artifact_path,
            "spark": spark,
            "catalog": catalog,
            "schema": schema,
            "run_id": run_id,
            "iteration": iteration,
        }

        # Safety: prevent generated code from stopping the shared SparkSession.
        # Some generated notebooks/scripts call `spark.stop()`. In Databricks jobs,
        # that can tear down the active SparkContext and cause follow-on failures
        # (e.g., telemetry writes via spark.sql) with errors like:
        # "SparkSession does not exist in the JVM" / "no active spark context".
        try:
            original_stop = getattr(spark, "stop", None)

            def _no_op_stop(*args, **kwargs):  # noqa: ANN001
                logger.warning(
                    "Generated code attempted to call spark.stop(); ignoring to keep executor alive."
                )

            if callable(original_stop):
                spark.stop = _no_op_stop  # type: ignore[method-assign]
        except Exception:
            # If monkeypatching fails, continue; worst case the code can still stop Spark.
            logger.debug("Could not patch spark.stop()", exc_info=True)
        
        # Add commonly used imports to the execution environment
        exec_globals["os"] = os
        exec_globals["sys"] = sys
        exec_globals["json"] = json
        exec_globals["datetime"] = datetime
        exec_globals["Path"] = Path
        
        # Execute the artifact with stdout/stderr capture
        logger.info("Executing artifact code (capturing stdout/stderr)...")
        with redirect_stdout(stdout_buffer), redirect_stderr(stderr_buffer):
            exec(code, exec_globals)
        
        # Check for a result variable in the execution context
        output = exec_globals.get("result", exec_globals.get("output", None))
        
        result["status"] = "success"
        result["output"] = _serialize_output(output)
        logger.info("Artifact execution completed successfully")
        
    except Exception as e:
        result["status"] = "failed"
        result["error"] = str(e)
        result["error_trace"] = traceback.format_exc()
        logger.error(f"Artifact execution failed: {e}")
        logger.error(result["error_trace"])
    
    finally:
        end_time = datetime.now(timezone.utc)
        result["end_time"] = end_time.isoformat()
        result["duration_seconds"] = (end_time - start_time).total_seconds()
        
        # Capture stdout/stderr content
        result["stdout"] = stdout_buffer.getvalue() or None
        result["stderr"] = stderr_buffer.getvalue() or None
        
        # Log captured output for debugging
        if result["stdout"]:
            logger.info(f"Captured stdout ({len(result['stdout'])} chars):")
            for line in result["stdout"].split('\n')[:10]:  # Log first 10 lines
                logger.info(f"  | {line}")
            if result["stdout"].count('\n') > 10:
                logger.info(f"  | ... ({result['stdout'].count(chr(10)) - 10} more lines)")
        
        if result["stderr"]:
            logger.warning(f"Captured stderr ({len(result['stderr'])} chars):")
            for line in result["stderr"].split('\n')[:10]:
                logger.warning(f"  | {line}")
    
    # Write result.json
    result_path = _write_result_json(
        artifact_path=artifact_path,
        result=result,
        run_id=run_id,
        iteration=iteration,
    )
    result["result_json_path"] = result_path
    
    return result


def _serialize_output(output: Any) -> Any:
    """Serialize execution output to a JSON-compatible format.
    
    Args:
        output: The output to serialize.
        
    Returns:
        JSON-serializable representation of the output.
    """
    if output is None:
        return None
    
    # Handle common types
    if isinstance(output, (str, int, float, bool)):
        return output
    
    if isinstance(output, (list, tuple)):
        return [_serialize_output(item) for item in output]
    
    if isinstance(output, dict):
        return {str(k): _serialize_output(v) for k, v in output.items()}
    
    # Try to convert to string for other types
    try:
        return str(output)
    except Exception:
        return f"<non-serializable: {type(output).__name__}>"


def _write_result_json(
    artifact_path: str,
    result: dict[str, Any],
    run_id: str,
    iteration: int,
) -> Optional[str]:
    """Write result.json to the same directory as the artifact.
    
    Args:
        artifact_path: Path to the executed artifact.
        result: The execution result dictionary.
        run_id: The run identifier.
        iteration: The iteration number.
        
    Returns:
        Path to the result.json file, or None if write failed.
    """
    try:
        # Determine result file path
        artifact_dir = os.path.dirname(artifact_path)
        if not artifact_dir:
            artifact_dir = os.environ.get(
                "ADK_ARTIFACTS_PATH",
                "/Volumes/silo_dev_rs/adk/artifacts"
            )
        
        # Create result filename with run_id and iteration
        result_filename = f"result_{run_id}_iter{iteration}.json"
        result_path = os.path.join(artifact_dir, result_filename)
        
        # Ensure directory exists
        os.makedirs(artifact_dir, exist_ok=True)
        
        # Write result
        with open(result_path, 'w') as f:
            json.dump(result, f, indent=2, default=str)
        
        logger.info(f"Result written to: {result_path}")
        return result_path
        
    except Exception as e:
        logger.error(f"Failed to write result.json: {e}")
        return None


def load_result_json(result_path: str) -> Optional[dict[str, Any]]:
    """Load a result.json file.
    
    Args:
        result_path: Path to the result.json file.
        
    Returns:
        The result dictionary, or None if not found.
    """
    try:
        with open(result_path, 'r') as f:
            return json.load(f)
    except FileNotFoundError:
        logger.warning(f"Result file not found: {result_path}")
        return None
    except json.JSONDecodeError as e:
        logger.error(f"Invalid JSON in result file: {e}")
        return None


def find_result_json(
    artifacts_path: str,
    run_id: str,
    iteration: int,
) -> Optional[str]:
    """Find a result.json file for a specific run/iteration.

    Args:
        artifacts_path: Base artifacts directory.
        run_id: The run identifier.
        iteration: The iteration number.

    Returns:
        Path to the result.json file, or None if not found.
    """
    result_filename = f"result_{run_id}_iter{iteration}.json"
    result_path = os.path.join(artifacts_path, result_filename)

    if os.path.exists(result_path):
        return result_path

    return None


# =============================================================================
# RLM Workflow Support
# =============================================================================

# RLM output markers for log parsing
RLM_EXEC_START_MARKER = "===RLM_EXEC_START"
RLM_EXEC_END_MARKER = "===RLM_EXEC_END"


def execute_from_registry(
    spark: "SparkSession",
    artifact_id: str,
    catalog: str,
    schema: str,
    timeout_seconds: int = 3600,
) -> dict[str, Any]:
    """Execute an artifact by loading it from the artifact registry.

    This function is designed for the RLM workflow where code is stored
    in the artifact registry Delta table rather than directly in a file path.

    The output is wrapped in RLM markers for reliable log parsing:
        ===RLM_EXEC_START artifact_id={id}===
        <agent code output>
        ===RLM_EXEC_END artifact_id={id} status={success|failed}===

    Args:
        spark: SparkSession for execution context.
        artifact_id: The artifact identifier in the registry.
        catalog: Unity Catalog name.
        schema: Schema name.
        timeout_seconds: Maximum execution time (not enforced in this version).

    Returns:
        Dict with execution results including status, stdout, stderr, etc.
    """
    from databricks_rlm_agent.artifact_registry import get_artifact_registry

    start_time = datetime.now(timezone.utc)

    logger.info(f"Executing artifact from registry: {artifact_id}")

    # Print start marker for log parsing
    print(f"{RLM_EXEC_START_MARKER} artifact_id={artifact_id}===")

    result = {
        "status": "pending",
        "artifact_id": artifact_id,
        "start_time": start_time.isoformat(),
        "end_time": None,
        "duration_seconds": None,
        "stdout": None,
        "stderr": None,
        "output": None,
        "error": None,
        "error_trace": None,
    }

    # Create buffers for capturing stdout/stderr
    stdout_buffer = io.StringIO()
    stderr_buffer = io.StringIO()

    try:
        # Load artifact from registry
        registry = get_artifact_registry(spark, catalog, schema, ensure_exists=False)
        artifact = registry.get_artifact(artifact_id)

        if not artifact:
            raise ValueError(f"Artifact not found in registry: {artifact_id}")

        result["run_id"] = artifact.session_id
        result["iteration"] = artifact.iteration

        # Get code - either from artifact_key or from embedded code
        code = None

        # Try loading from code_artifact_key if available
        # Note: In full implementation, this would use ArtifactService
        # For now, check if there's a file at the expected path
        if artifact.code_artifact_key:
            artifacts_path = os.environ.get(
                "ADK_ARTIFACTS_PATH",
                "/Volumes/silo_dev_rs/adk/artifacts"
            )
            code_path = os.path.join(artifacts_path, artifact.code_artifact_key)
            if os.path.exists(code_path):
                with open(code_path, 'r') as f:
                    code = f.read()
                logger.info(f"Loaded code from artifact key: {code_path}")

        if not code:
            raise ValueError(f"Could not load code for artifact: {artifact_id}")

        logger.info(f"Artifact loaded, {len(code)} characters")

        # Prepare execution environment
        exec_globals = {
            "__name__": "__main__",
            "__file__": f"artifact_{artifact_id}.py",
            "spark": spark,
            "catalog": catalog,
            "schema": schema,
            "artifact_id": artifact_id,
            "run_id": artifact.session_id,
            "iteration": artifact.iteration,
        }

        # Add commonly used imports
        exec_globals["os"] = os
        exec_globals["sys"] = sys
        exec_globals["json"] = json
        exec_globals["datetime"] = datetime
        exec_globals["Path"] = Path

        # Execute with stdout/stderr capture
        logger.info("Executing artifact code (capturing stdout/stderr)...")
        with redirect_stdout(stdout_buffer), redirect_stderr(stderr_buffer):
            exec(code, exec_globals)

        # Check for result variable
        output = exec_globals.get("result", exec_globals.get("output", None))

        result["status"] = "success"
        result["output"] = _serialize_output(output)
        logger.info("Artifact execution completed successfully")

    except Exception as e:
        result["status"] = "failed"
        result["error"] = str(e)
        result["error_trace"] = traceback.format_exc()
        logger.error(f"Artifact execution failed: {e}")
        logger.error(result["error_trace"])

    finally:
        end_time = datetime.now(timezone.utc)
        result["end_time"] = end_time.isoformat()
        result["duration_seconds"] = (end_time - start_time).total_seconds()

        # Capture stdout/stderr
        result["stdout"] = stdout_buffer.getvalue() or None
        result["stderr"] = stderr_buffer.getvalue() or None

        # Print captured output (will be between markers)
        if result["stdout"]:
            print(result["stdout"])

        if result["stderr"]:
            print(f"STDERR: {result['stderr']}", file=sys.stderr)

        # Print end marker
        print(f"{RLM_EXEC_END_MARKER} artifact_id={artifact_id} status={result['status']}===")

        # Log summary
        if result["stdout"]:
            logger.info(f"Captured stdout ({len(result['stdout'])} chars)")
        if result["stderr"]:
            logger.warning(f"Captured stderr ({len(result['stderr'])} chars)")

    # Update artifact registry with results
    try:
        registry = get_artifact_registry(spark, catalog, schema, ensure_exists=False)
        registry.update_artifact(
            artifact_id=artifact_id,
            status=result["status"],
            metadata={
                "stdout_length": len(result["stdout"]) if result["stdout"] else 0,
                "stderr_length": len(result["stderr"]) if result["stderr"] else 0,
                "duration_seconds": result["duration_seconds"],
            }
        )
    except Exception as e:
        logger.warning(f"Failed to update artifact registry: {e}")

    return result


def print_rlm_markers(artifact_id: str, content: str, status: str = "success") -> None:
    """Print content wrapped in RLM markers for log parsing.

    Args:
        artifact_id: The artifact identifier.
        content: The content to print between markers.
        status: The execution status (success/failed).
    """
    print(f"{RLM_EXEC_START_MARKER} artifact_id={artifact_id}===")
    print(content)
    print(f"{RLM_EXEC_END_MARKER} artifact_id={artifact_id} status={status}===")

