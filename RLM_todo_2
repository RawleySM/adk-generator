# RLM TODO: Sync `plans/RLM_overview.md` vision with `databricks_rlm_agent/` reality

This is a concrete, implementation-oriented checklist to close the gap between:

- **Vision**: “two-job RLM loop + downstream SubLM digestion” (`plans/RLM_overview.md`)
- **Reality**: Job_A/Job_B/Job_C scaffolding exists, but **`llm_query` is a file-write**, the “REPL” described in prompts does not exist, and there is **no downstream-output digestion pipeline**.

This plan assumes we keep the “two-job” pattern (Job_A orchestrator + Job_B executor) and make `llm_query` the gateway that hands work to a downstream `llm_query_agent` after the executor run.

---

## 0) Non-negotiable fixes (stop the bleeding first)

- **Fix the core semantic lie**
  - **Current**: `prompts.py` tells the model `llm_query()` queries a downstream LLM.
  - **Reality**: `tools/llm_query.py` only writes to `AGENT_CODE_PATH`.
  - **TODO**: Update the prompt (or tool) so the model’s mental model matches reality.
  - **Acceptance**: Model no longer expects `llm_query()` to return analysis text unless it actually does.

- **Split artifact paths**
  - **Current**: `save_python_code` and `llm_query` both write to `AGENT_CODE_PATH`.
  - **TODO**: Separate paths/env vars for:
    - executable “Job_B code” (e.g. `ADK_EXEC_CODE_PATH`)
    - “SubLM request payload” (e.g. `ADK_LLM_QUERY_PATH`)
  - **Acceptance**: Calling `llm_query()` can’t overwrite the executor artifact.

---

## 1) Clarify the contract: what happens after `databricks_analyst` calls `llm_query(code_blob)`

### Required behavior (the missing contract)

When `databricks_analyst` submits a `code_blob` into the `llm_query` tool:

1. The submission is **validated** (safety + formatting + lint).
2. The tool **extracts**:
   - **`sublm_instruction`** (the “query”, from a structured docstring/header)
   - **`agent_code`** (the executable code portion)
3. The tool **persists** both to a Unity Catalog Delta “artifact registry” table (per-session/per-iteration).
4. The tool **signals** the current agent turn should stop, allowing the orchestrator to:
   - submit Job_B to execute `agent_code`
   - then move control to `llm_query_agent` with the execution results + instruction loaded

### IMPORTANT correction to your draft

- In this repo’s ADK usage, the reliable stop signal is `tool_context.actions.escalate = True` (see `exit_loop` and `LlmQueryEscalationPlugin`).
- The field `tool_context.invocation_context.end_invocation = True` is **not used anywhere in this codebase** and likely isn’t a supported/public ADK API surface.
- **TODO**: Standardize on `tool_context.actions.escalate` to end the current loop/turn.

---

## 2) Implement the validation pipeline before `llm_query`

You proposed a chain of checks before `llm_query` actually runs. Concretely, implement this as ADK plugins (before_tool callbacks), wired into the `Runner` plugins list.

### 2.1 Safety gate (already exists, not wired consistently)

- **File**: `databricks_rlm_agent/plugins/uc_tool_execution_safety_plugin.py`
- **TODO**
  - Wire `UcToolExecutionSafetyPlugin` into the actual runtime plugin list used by `Runner` in `run.py` (and/or by `App` in `agent.py`).
  - Ensure it runs for `llm_query` and `save_python_code`.
- **Acceptance**
  - A clearly dangerous payload is blocked and recorded (stdout + telemetry).

### 2.2 `formatting_check_before_tool_callback` (new)

- **Goal**: enforce that the submission is parseable (docstring header + code).
- **TODO**
  - Create a new plugin or local callback class (prefer plugin for consistency) that:
    - asserts `code_blob` is `str`
    - asserts it contains a required docstring block at the top with a strict schema (example below)
    - fails closed (blocks tool execution) with a structured error dict returned to the agent
- **Docstring schema (example)**
  - First lines of the blob must be:
    - triple-quoted docstring
    - contains `RLM_QUERY:` and optionally `RLM_CONTEXT:` and `RLM_OUTPUT_EXPECTATION:`
- **Acceptance**
  - Bad formats are blocked early with actionable error messages.

### 2.3 `code_linting_before_tool_callback` (new)

- **Goal**: catch obviously broken code before burning a Job_B run.
- **TODO**
  - Implement a plugin that:
    - parses the extracted code with `ast.parse` (syntax check)
    - optionally runs `ruff` or `pyflakes` if available in the wheel env (or a minimal internal lint)
    - blocks `llm_query` if checks fail
- **Acceptance**
  - Syntax errors are blocked before Job_B submission; error returned includes exact location.

---

## 3) Build the “artifact registry” Delta table (missing)

You want `llm_query()` to write `sublm_instruction` and `agent_code` into a UC Delta registry managed alongside session persistence.

### 3.1 Decide ownership and location

Two viable designs:

- **A) Separate registry service** (recommended): new module `artifact_registry.py`
  - Keeps `DeltaSessionService` focused on sessions/events/state.
- **B) Extend `DeltaSessionService`** to include artifact registry helpers
  - Works, but increases surface area of an already-large file.

### 3.2 Define registry schema (minimum)

- **Table**: `{catalog}.{schema}.artifact_registry`
- **Columns (minimum)**
  - `artifact_id` (string, UUID)
  - `session_id` (string)
  - `invocation_id` (string, optional)
  - `iteration` (int)
  - `created_ts` (timestamp)
  - `artifact_type` (string enum: `llm_query_request`, `executor_stdout`, `executor_stderr`, `executor_result_json`, `sublm_response`, etc.)
  - `sublm_instruction` (string, nullable)
  - `agent_code` (string, nullable)
  - `stdout` (string, nullable)
  - `stderr` (string, nullable)
  - `metadata_json` (string)

### 3.3 Implement CRUD helpers

- **TODO**
  - `ensure_artifact_registry_table(spark, catalog, schema)`
  - `insert_llm_query_request(session_id, iteration, sublm_instruction, agent_code, ...) -> artifact_id`
  - `update_executor_results(artifact_id, stdout, stderr, status, error, ...)`
  - `load_request(artifact_id)` / `load_executor_results(artifact_id)`
- **Acceptance**
  - You can insert a request and later attach executor results by `artifact_id`.

---

## 4) Rebuild `llm_query()` as “parse + persist + signal”

### 4.1 Parse docstring header into `sublm_instruction`

- **TODO**
  - Implement parsing rules:
    - locate top-of-file docstring (strict)
    - extract `RLM_QUERY:` body as the downstream LLM instruction
    - extract code body (everything after docstring)
  - Return structured parse errors when missing/malformed.
- **Acceptance**
  - A valid blob yields `(sublm_instruction, agent_code)` deterministically.

### 4.2 Persist to artifact registry + state keys

- **TODO**
  - Write `sublm_instruction` and `agent_code` to the registry table as a new `artifact_id`.
  - Set state keys for downstream retrieval, e.g.:
    - `tool_context.state["rlm.last_artifact_id"] = artifact_id`
    - `tool_context.state["rlm.sublm_instruction_key"] = "rlm.sublm_instruction"` (or store the text directly if small)
- **Acceptance**
  - `llm_query` returns `{status, artifact_id, ...}` and the state contains the pointer(s).

### 4.3 Signal “stop this agent / let orchestrator proceed”

- **TODO**
  - Set `tool_context.actions.escalate = True` (consistent with existing code).
- **Acceptance**
  - After calling `llm_query`, `databricks_analyst` does not keep looping in-place.

---

## 5) Deterministic “job_builder” / “job_submitter” component

You described a deterministic component that:

- builds a Job_B job JSON
- submits with `jobs.run_now`
- waits + reads run output
- writes results back to registry
- then yields control to `llm_query_agent`

### 5.1 Choose the implementation shape (don’t invent a new control plane unnecessarily)

There are two viable paths:

- **Path A (recommended): keep using `cli.py` orchestrator loop**
  - You already have Job submission and waiting in `databricks_rlm_agent/cli.py` + `jobs_api.py`.
  - Extend that code to submit Job_B by **artifact_id** (registry row key), not by “a local file path”.

- **Path B: implement a dedicated deterministic `job_builder` BaseAgent**
  - This is additional moving parts and should only happen if ADK sequencing requirements force it.

**TODO**: pick Path A unless we find a hard ADK limitation.

### 5.2 Job_B interface: pass `artifact_id`, not `artifact_path`

- **Current**: Job_B takes `ARTIFACT_PATH` and runs whatever file is there.
- **Target**: Job_B takes `ARTIFACT_ID` (registry key), loads code from Delta, executes it, writes stdout/stderr back to the registry.

**TODO**
- Update `jobs_api.submit_executor_run(...)` to pass:
  - `ARTIFACT_ID` (new)
  - `RUN_ID`, `ITERATION`, `ADK_DELTA_CATALOG`, `ADK_DELTA_SCHEMA`
- Update `cli.py` orchestrator to submit using `ARTIFACT_ID` returned by `llm_query()`.

**Acceptance**
- There is a single stable identifier that ties together:
  - the request (`sublm_instruction` + `agent_code`)
  - the executor outputs
  - the SubLM response

---

## 6) Implement Job_B “executor_runner.py” (the real payload runner)

Your intent is correct: Job_B should run a deterministic runner script that:

- loads the requested `agent_code` by `artifact_id`
- executes it
- captures stdout/stderr
- writes outputs back to the same registry row

### 6.1 Where to put it

- **TODO**: Add `databricks_rlm_agent/executor_runner.py` into the wheel, so Job_B can execute it as a `python_wheel_task` entrypoint (simplest).
- Avoid relying on an external `/Volumes/.../executor_runner.py` path during early stabilization.

### 6.2 What it must print (for Jobs API log parsing)

Even if we persist stdout to Delta, it’s still useful to have log markers.

- **TODO**: Print deterministic markers:
  - `RLM_EXEC_START artifact_id=<id>`
  - `RLM_EXEC_END artifact_id=<id> status=<success|failed>`

**Acceptance**
- `jobs.get_run_output().logs` can be parsed safely even if stdout is large.

---

## 7) Update the executor implementation to run code from registry

### 7.1 Extend `databricks_rlm_agent/executor.py`

- **Current**: `execute_artifact(spark, artifact_path, ...)` reads a file and `exec()`s it.
- **Target**: `execute_registry_artifact(spark, artifact_id, catalog, schema, ...)` loads code string from Delta and executes it.

**TODO**
- Add a new entrypoint in `executor.py` for registry-based execution.
- Keep the file-based execution temporarily for backward compatibility, but do not use it in the “vision path”.

**Acceptance**
- A given `artifact_id` can be executed in Job_B without any filesystem writes from Job_A.

### 7.2 Persist results back to registry

**TODO**
- `update_executor_results(artifact_id, stdout, stderr, status, error, error_trace, duration, ...)`
- Optionally store a small “execution summary” column to prevent downstream agents from ingesting raw walls of text by default.

---

## 8) Orchestrator: consume executor output and transition control to `llm_query_agent`

This is the heart of the “downstream SubLM digestion” story.

### 8.1 Don’t rely on implicit agent switching

Right now, there is no implemented mechanism that says:

> “after `databricks_analyst` calls tool X, automatically run `llm_query_agent` next with Y context”

So we must implement it explicitly.

### 8.2 Inject `sublm_instruction` + `stdout/stderr` into `llm_query_agent`

Because `llm_query_agent` currently has no tools, the cleanest approach is an ADK plugin that:

- on `before_agent_callback` for agent `llm_query`
  - reads `artifact_id` from state (or from the latest session record)
  - loads `sublm_instruction` + executor outputs from the registry
  - returns a `types.Content` (user message) that contains:
    - the instruction
    - the outputs (or a truncated + link/pointer, depending on size)

**TODO**
- Implement a plugin like `RlmLlqContextInjectionPlugin`.
- Wire it into the Runner’s plugins list.

**Acceptance**
- `llm_query_agent` gets the *actual* instruction and outputs, without “pretend REPL” prompt hacks.

---

## 9) Prune/clear transient state after `llm_query_agent` completes

Your “prune_context_callback” idea is correct, but implement it using the ADK surfaces already used in this repo:

- Use `after_agent_callback` to clear state keys, and/or mark registry rows as “consumed”.

**TODO**
- Implement `after_agent_callback` hook for agent `llm_query` to:
  - clear `tool_context.state["rlm.last_artifact_id"]` (or equivalent)
  - optionally write `artifact_type='sublm_response'` row or attach `sublm_response` to the same `artifact_id`

**Acceptance**
- The next iteration doesn’t accidentally reuse stale stdout/instructions.

---

## 10) Reconcile with `cli.py` “iteration loop”

Currently `cli.py` loops by feeding `format_execution_feedback(...)` (stdout/stderr/error) back into the *same* agent.

That is explicitly what the vision is trying to avoid.

**TODO**
- Replace “feed raw executor output back into `databricks_analyst`” with:
  - execute → store outputs → run `llm_query_agent` to digest → feed *summary* back into `databricks_analyst`

**Acceptance**
- `databricks_analyst` sees compact summaries (and pointers), not giant logs.

---

## 11) Telemetry and observability (make failures obvious)

**TODO**
- Add a small set of hard event types written to `telemetry.py` and/or `adk_telemetry`:
  - `llm_query_request_saved`
  - `executor_started`, `executor_completed`
  - `llm_query_agent_started`, `llm_query_agent_completed`
  - `artifact_registry_write_failed`
- Ensure every stage writes `artifact_id`, `session_id`, `iteration`.

**Acceptance**
- When things “get stuck”, you can pinpoint exactly which step stalled by querying a Delta table.

---

## 12) Test plan (minimum to avoid regressions)

**TODO**
- Unit tests for:
  - docstring parsing → `(sublm_instruction, agent_code)`
  - formatting/lint plugin behavior (block vs allow)
  - artifact registry insert + update roundtrip
- “Dry run” integration test (no Databricks) that:
  - simulates `llm_query()` writing a request row
  - runs executor function locally against registry-loaded code
  - confirms results are written back to registry

**Acceptance**
- You can validate 80% of the pipeline in CI without running a Databricks job.


